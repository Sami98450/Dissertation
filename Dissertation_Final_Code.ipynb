{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PFzRVgAaf3t"
      },
      "source": [
        "# **Importing** **Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yh8FBIfmbjQC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3362e251-1655-44dd-c154-659b1d78ab21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported.\n"
          ]
        }
      ],
      "source": [
        "#Importing all required libraries and packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import seaborn as sb\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from statsmodels.graphics import tsaplots\n",
        "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\n",
        "import math\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from math import floor\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "from sklearn import model_selection\n",
        "from math import sqrt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#from windrose import WindroseAxes\n",
        "import matplotlib.cm as cm\n",
        "from statsmodels.graphics import tsaplots\n",
        "print('Libraries imported.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwN5oQ8Valzz"
      },
      "source": [
        "# **Read** **data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e1rq5iOAhKv"
      },
      "outputs": [],
      "source": [
        "#Read Traffic Data From Sharston\n",
        "traffic22 = pd.read_csv('pvr_2016-03-04_1765d(1).csv')\n",
        "#Rename date column to match the date column of the Pollutants Data from Piccadilly\n",
        "traffic22 = traffic22.rename(columns = {\"Sdate\":\"date\"})\n",
        "#Rename Cosit to match the same style as well\n",
        "traffic22['Cosit'].replace('=\"MAC030001146\"', 'MAC030001146', inplace=True)\n",
        "#Remove Channel 1 as it recorded no Traffic volume\n",
        "traffic22 = traffic22[traffic22.LaneDescription != 'Channel 2']\n",
        "traffic22.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8kdQiuEdF1p"
      },
      "outputs": [],
      "source": [
        "#Read Pollutants Data for Piccadilly\n",
        "PollutantsPicc = pd.read_csv('Piccadilly.csv')\n",
        "PollutantsPicc.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Read Data from Sharston\n",
        "PollutantsShar = pd.read_csv('Sharston.csv')\n",
        "PollutantsShar.head()"
      ],
      "metadata": {
        "id": "D3qkhtOTCLfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read traffic data for Sharston \n",
        "traffic3 = pd.read_csv('pvr_2015-06-01_2615d.csv')\n",
        "#Rename date column to match pollutants data\n",
        "traffic3 = traffic3.rename(columns = {\"Sdate\":\"date\"})\n",
        "#Remove Channel 1 as it recorded no Traffic volume values\n",
        "traffic3 = traffic3[traffic3.LaneDescription != 'Channel 2']"
      ],
      "metadata": {
        "id": "kFWegkNzCMFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Piccadilly site**"
      ],
      "metadata": {
        "id": "3BWdbgujbiku"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPNNqTk57k4S"
      },
      "source": [
        "## **Feature Engineering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQPFtVWnlfR3"
      },
      "outputs": [],
      "source": [
        "#merge traffic data and pollutants data on the date column\n",
        "df = pd.merge(traffic22,PollutantsPicc[['date','NO2','wd','ws','temp','longitude', 'latitude']],on='date', how='left')\n",
        "#drop irrelevant variables\n",
        "df = df.drop(['AvgSpeed', 'PmlHGV', 'Flag Text'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check for null values\n",
        "df.isnull().mean()"
      ],
      "metadata": {
        "id": "FaI-gG4b9Z9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop null values\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "dEF5Ded49TLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check correlation between variables\n",
        "df[['NO2','Volume','ws','wd','temp']].corr()"
      ],
      "metadata": {
        "id": "eh3R8NPYkYKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#summary statistics for some variables\n",
        "df[['NO2','Volume','ws','wd','temp']].describe()"
      ],
      "metadata": {
        "id": "c_E1WA2vcStn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tj4GRql7jK-S"
      },
      "outputs": [],
      "source": [
        "#extract seasonality data drom data column\n",
        "df['date'] =  pd.to_datetime(df['date'])\n",
        "df['Year'] = df['date'].dt.year\n",
        "df['Month'] = df['date'].dt.month\n",
        "df['Hour'] = df['date'].dt.hour\n",
        "df['DayofWeek'] = df['date'].dt.dayofweek\n",
        "df['dayofyear'] = df['date'].dt.dayofyear\n",
        "df['weekofyear'] = df['date'].dt.weekofyear\n",
        "df['quarter'] = df['date'].dt.quarter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create new variables using rolling means function\n",
        "df['NO2_moving_avg'] = df['NO2'].rolling(window=3).mean()\n",
        "df['NO2_moving_avg2'] = df['NO2'].rolling(window=25).mean()\n",
        "df['NO2_moving_avg3'] = df['NO2'].rolling(window=50).mean()"
      ],
      "metadata": {
        "id": "BykH7NI81DhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop any new missing values\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "u9xas3UDpTso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transform wind direction from degrees to textual direction\n",
        "df['wd'] = df['wd'].mask(df['wd'] == 0, 0)\n",
        "df['wd'] = df['wd'].mask((df['wd'] > 0) & (df['wd'] < 90), 0.5)\n",
        "df['wd'] = df['wd'].mask(df['wd'] == 90, 1)\n",
        "df['wd'] = df['wd'].mask((df['wd'] > 90) & (df['wd'] < 180), 1.5)\n",
        "df['wd'] = df['wd'].mask(df['wd'] == 180, 2)\n",
        "df['wd'] = df['wd'].mask((df['wd'] > 180) & (df['wd'] < 270), 2.5)\n",
        "df['wd'] = df['wd'].mask(df['wd'] == 270, 3)\n",
        "df['wd'] = df['wd'].mask((df['wd'] > 270) & (df['wd'] < 360), 3.5)\n",
        "df['wd'] = df['wd'].mask(df['wd'] == 360, 'North')\n",
        "\n",
        "df['wd'] = df['wd'].mask(df['wd'] == 0, 'North')\n",
        "df['wd'] = df['wd'].mask(df['wd'] == 0.5, 'North East')\n",
        "df['wd'] = df['wd'].mask(df['wd'] == 1, 'East')\n",
        "df['wd'] = df['wd'].mask(df['wd'] == 1.5, 'South East')\n",
        "df['wd'] = df['wd'].mask(df['wd'] == 2, 'South')\n",
        "df['wd'] = df['wd'].mask(df['wd'] == 2.5, 'South West')\n",
        "df['wd'] = df['wd'].mask(df['wd'] == 3, 'West')\n",
        "df['wd'] = df['wd'].mask(df['wd'] == 3.5, 'North West')"
      ],
      "metadata": {
        "id": "kwLIa4UROpiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#One hot encode Wind direction\n",
        "#creating instance of one-hot-encoder\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "#perform one-hot encoding on 'wd' column \n",
        "encoder_df = pd.DataFrame(encoder.fit_transform(df[['wd1']]).toarray())\n",
        "\n",
        "#merge one-hot encoded columns back with original DataFrame\n",
        "df = df.join(encoder_df)"
      ],
      "metadata": {
        "id": "XDvER4lBPUPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating instance of one-hot-encoder\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "#perform one-hot encoding on 'Day of week' column \n",
        "df[['ohe1','ohe2','ohe3','oh4','o5','o6','o7']] = encoder.fit_transform(df[['DayofWeek']]).toarray()"
      ],
      "metadata": {
        "id": "7NTiDuNBPZhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pollutants"
      ],
      "metadata": {
        "id": "dz2_LL9qPLXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#drop irrelevant variables\n",
        "PollutantsPicc = PollutantsPicc.drop(['NOXasNO2', 'SO2', 'NV2.5',\n",
        "                                      'V2.5','AT2.5','AP2.5','AT25','AP25',\n",
        "                                      'PM10','RAWPM25'], axis = 1)"
      ],
      "metadata": {
        "id": "rPXD9VI7QgMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check for missing values\n",
        "PollutantsPicc.isnull().mean()"
      ],
      "metadata": {
        "id": "ii_PzaPy1G0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#correlation for data\n",
        "PollutantsPicc[['NO2', 'ws','wd','temp']].corr()"
      ],
      "metadata": {
        "id": "nlOukuWw-gfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Summary statistics for data\n",
        "PollutantsPicc[['NO2', 'ws','wd','temp']].describe()"
      ],
      "metadata": {
        "id": "pky1tHxI1SUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transform wind direction from degrees to textual direction\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask(PollutantsPicc['wd'] == 0, 0)\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask((PollutantsPicc['wd'] > 0) & (PollutantsPicc['wd'] < 90), 0.5)\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask(PollutantsPicc['wd'] == 90, 1)\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask((PollutantsPicc['wd'] > 90) & (PollutantsPicc['wd'] < 180), 1.5)\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask(PollutantsPicc['wd'] == 180, 2)\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask((PollutantsPicc['wd'] > 180) & (PollutantsPicc['wd'] < 270), 2.5)\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask(PollutantsPicc['wd'] == 270, 3)\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask((PollutantsPicc['wd'] > 270) & (PollutantsPicc['wd'] < 360), 3.5)\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask(PollutantsPicc['wd'] == 360, 'North')\n",
        "\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask(PollutantsPicc['wd'] == 0, 'North')\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask(PollutantsPicc['wd'] == 0.5, 'North East')\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask(PollutantsPicc['wd'] == 1, 'East')\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask(PollutantsPicc['wd'] == 1.5, 'South East')\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask(PollutantsPicc['wd'] == 2, 'South')\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask(PollutantsPicc['wd'] == 2.5, 'South West')\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask(PollutantsPicc['wd'] == 3, 'West')\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask(PollutantsPicc['wd'] == 3.5, 'North West')"
      ],
      "metadata": {
        "id": "6YoGDC7p4LUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#creating instance of one-hot-encoder\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "#perform one-hot encoding on 'wd' column \n",
        "encoder_df = pd.DataFrame(encoder.fit_transform(PollutantsPicc[['wd']]).toarray())\n",
        "\n",
        "#merge one-hot encoded columns back with original DataFrame\n",
        "PollutantsPicc = PollutantsPicc.join(encoder_df)\n",
        "\n"
      ],
      "metadata": {
        "id": "yd6eHg4f7J78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extract new features from date column\n",
        "PollutantsPicc['date'] =  pd.to_datetime(PollutantsPicc['date'])\n",
        "PollutantsPicc['Year'] = PollutantsPicc['date'].dt.year\n",
        "PollutantsPicc['Month'] = PollutantsPicc['date'].dt.month\n",
        "PollutantsPicc['Hour'] = PollutantsPicc['date'].dt.hour\n",
        "PollutantsPicc['DayofWeek'] = PollutantsPicc['date'].dt.dayofweek\n",
        "PollutantsPicc['dayofyear'] = PollutantsPicc['date'].dt.dayofyear\n",
        "PollutantsPicc['weekofyear'] = PollutantsPicc['date'].dt.weekofyear\n",
        "PollutantsPicc['quarter'] = PollutantsPicc['date'].dt.quarter\n",
        "#create new variables using rolling means\n",
        "PollutantsPicc['NO2_moving_avg'] = PollutantsPicc['NO2'].rolling(window=3).mean()#3hours\n",
        "PollutantsPicc['NO2_moving_avg2'] = PollutantsPicc['NO2'].rolling(window=25).mean()#24hours\n",
        "PollutantsPicc['NO2_moving_avg3'] = PollutantsPicc['NO2'].rolling(window=50).mean()#~2days\n",
        "PollutantsPicc['NO2_moving_avg4'] = PollutantsPicc['NO2'].rolling(window=100).mean()#~4days\n",
        "PollutantsPicc['NO2_moving_avg5'] = PollutantsPicc['NO2'].rolling(window=200).mean()#~8days"
      ],
      "metadata": {
        "id": "NLn4T0JMQFsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#creating instance of one-hot-encoder\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "#perform one-hot encoding on 'dayofweek' column \n",
        "PollutantsPicc[['ohe1','ohe2','ohe3','oh4','o5','o6','o7']] = encoder.fit_transform(PollutantsPicc[['DayofWeek']]).toarray()\n",
        "\n",
        "#merge one-hot encoded columns back with original DataFrame\n",
        "#PollutantsPicc = PollutantsPicc.join(encoder_df)"
      ],
      "metadata": {
        "id": "cOVDXmH76fCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop missing values\n",
        "PollutantsPicc = PollutantsPicc.dropna()"
      ],
      "metadata": {
        "id": "XMkJGirbRsnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PollutantsPicc.shape"
      ],
      "metadata": {
        "id": "dSiAwogXgZLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression"
      ],
      "metadata": {
        "id": "7zA4h6wdbvRt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8fTYm_p9PF"
      },
      "source": [
        "## **Prepare Data Where X contains the predictor variables and y the target**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PollutantsPicc.head(1)"
      ],
      "metadata": {
        "id": "U-K0Fsa8e-Z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PollutantsPicc.head(1)"
      ],
      "metadata": {
        "id": "0LxOjlNbgg8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb1"
      ],
      "metadata": {
        "id": "cy2KqVvbTXzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = PollutantsPicc.iloc[:, [5,6,7]].values\n",
        "y = PollutantsPicc.iloc[:,3].values"
      ],
      "metadata": {
        "id": "C6-XO7SS3RBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#onehotencoded\n",
        "X = PollutantsPicc.iloc[:, [6,7,14,15,16,17,18,19,20,21]].values\n",
        "y = PollutantsPicc.iloc[:,3].values"
      ],
      "metadata": {
        "id": "VH-7yh6yM-o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb2"
      ],
      "metadata": {
        "id": "Pgcq1Sh2TYlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = PollutantsPicc.iloc[:, [5,6,7,14,15,16,17,18,19,20]].values\n",
        "y = PollutantsPicc.iloc[:,3].values"
      ],
      "metadata": {
        "id": "Q9hjejo43mMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#onehotencoded\n",
        "X = PollutantsPicc.iloc[:, [6,7,14,15,16,17,18,19,20,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40]].values\n",
        "y = PollutantsPicc.iloc[:,3].values"
      ],
      "metadata": {
        "id": "vNmOKWgL6YFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb3"
      ],
      "metadata": {
        "id": "U_u66Qf1TZjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = df.iloc[:, [6,9,10,11,14,15,16,17,18,19,20]].values\n",
        "y = df.iloc[:,8].values"
      ],
      "metadata": {
        "id": "bRrJZAfx3tmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#onehotencoded\n",
        "X = df.iloc[:, [6,10,11,14,15,16,17,18,19,20,26,27,\n",
        "                28,29,30,31,32,33,34,35,36,37,38,39,40,41]].values\n",
        "y = df.iloc[:,8].values\n"
      ],
      "metadata": {
        "id": "1kcckEZdgM5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb4"
      ],
      "metadata": {
        "id": "m8DZdbuSTaQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = PollutantsPicc.iloc[:, [5,6,7,14,15,16,17,18,19,20]].values\n",
        "y = PollutantsPicc.iloc[:,22].values"
      ],
      "metadata": {
        "id": "uZTByLb7PnZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#onehotencoded\n",
        "X = PollutantsPicc.iloc[:, [6,7,14,15,16,17,,18,19,20,21,22,23,24,25,26,27,28,34,35,36,37,38,39,40,41]].values\n",
        "y = PollutantsPicc.iloc[:,29].values"
      ],
      "metadata": {
        "id": "TztBGggIR0X2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb5"
      ],
      "metadata": {
        "id": "zYuAtNyGTfDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = PollutantsPicc.iloc[:, [5,6,7,14,15,16,17,18,19,20]].values\n",
        "y = PollutantsPicc.iloc[:,23].values"
      ],
      "metadata": {
        "id": "S0xEIRg4JARp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#onehotencoded\n",
        "X = PollutantsPicc.iloc[:, [6,7,14,15,16,17,18,19,20,22,23,24,25,26,27,28,34,35,36,37,38,39,40,41]].values\n",
        "y = PollutantsPicc.iloc[:,30].values"
      ],
      "metadata": {
        "id": "hMtPQpJxR3u8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb6"
      ],
      "metadata": {
        "id": "lOj0Bgm7TfwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = PollutantsPicc.iloc[:, [5,6,7,14,15,16,17,18,19,20]].values\n",
        "y = PollutantsPicc.iloc[:,24].values"
      ],
      "metadata": {
        "id": "EKsQQ1TSJhYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#onehotencoded\n",
        "X = PollutantsPicc.iloc[:, [6,7,14,15,16,17,18,19,20,22,23,24,26,27,28,34,35,36,37,38,39,40,41]].values\n",
        "y = PollutantsPicc.iloc[:,31].values"
      ],
      "metadata": {
        "id": "HeNtN6nGBKHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb7"
      ],
      "metadata": {
        "id": "6Pm-RE4EThm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = df.iloc[:, [5,6,7,14,15,16,17,18,19,20]].values\n",
        "y = df.iloc[:,23].values"
      ],
      "metadata": {
        "id": "L31nCOhfoLK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#onehotencoded\n",
        "X = df.iloc[:, [6,7,14,15,16,17,18,19,20,22,23,24,26,27,28,34,35,36,37,38,39,40,41]].values\n",
        "y = df.iloc[:,32].values"
      ],
      "metadata": {
        "id": "Kc95LhJdR7RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perform Train Test Split"
      ],
      "metadata": {
        "id": "q7Rvxc0OXt_R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmqV7E3e8gND"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "print(scaler.fit(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qR6OWs9TloY2"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1,random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Random Forest**"
      ],
      "metadata": {
        "id": "m3fXf1ZiUNtY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0yRoLfRthB0"
      },
      "outputs": [],
      "source": [
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "print(random_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcztH4QXtnaw"
      },
      "outputs": [],
      "source": [
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "rf = RandomForestRegressor()\n",
        "# Random search of parameters, using 10 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n",
        "                               n_iter = 10, cv = 10, verbose=1, random_state=42, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "rf_random.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsCBbH4NPBL9"
      },
      "outputs": [],
      "source": [
        "#print the best parameters found\n",
        "rf_random.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run and fit the model\n",
        "rfregressor = RandomForestRegressor(n_estimators = 200,max_depth = 10, \n",
        "                                  max_features= 'sqrt', min_samples_split = 5, min_samples_leaf= 2,\n",
        "                                  bootstrap = True, random_state = 42)\n",
        "rfregressor.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "fiN0XtCq4obY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55BQBvE6PE1M"
      },
      "outputs": [],
      "source": [
        "#Run and fit the model\n",
        "rfregressor = RandomForestRegressor(n_estimators = 400,max_depth = 60, \n",
        "                                  max_features= 'sqrt', min_samples_split = 10, min_samples_leaf= 1,\n",
        "                                  bootstrap = False, random_state = 42)\n",
        "rfregressor.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8XbKzVR_F4p"
      },
      "outputs": [],
      "source": [
        "# Generates Predictions\n",
        "predictr=rfregressor.predict(X_test)\n",
        "predictr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the Algorithm\n",
        "print(rfregressor.score(X_train, y_train))\n",
        "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictr))  \n",
        "print('Mean Squared Error:', metrics.mean_squared_error(y_test, predictr))  \n",
        "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, \n",
        "                                                                     predictr)))\n",
        "print(\"R2: %.4f\" % r2_score(y_test, predictr))"
      ],
      "metadata": {
        "id": "_SQQIXZ8UxVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xdf = pd.DataFrame(X)\n",
        "Xf = list(Xdf.columns)\n",
        "Xdf = pd.DataFrame(X)\n",
        "Xf = list(Xdf.columns)\n",
        "# Get numerical feature importances\n",
        "importances = list(xgbr.feature_importances_)\n",
        "# List of tuples with variable and importance\n",
        "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(Xf, importances)]\n",
        "# Sort the feature importances by most important first\n",
        "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
        "# Print out the feature and importances \n",
        "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
      ],
      "metadata": {
        "id": "N4HctIcokyQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CV score\n",
        "scores = cross_val_score(rfregressor, X_train, y_train, \n",
        "                         scoring='r2', cv=10,n_jobs=-1)\n",
        "print(scores)\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "id": "p1O11Cr5unsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RF Plots "
      ],
      "metadata": {
        "id": "ikbjCk0ZAv6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a plot of observed vs predicted values on a log scale\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(y_test, predictr, c='crimson')\n",
        "plt.yscale('log')\n",
        "plt.xscale('log')\n",
        "\n",
        "p1 = max(max(predictr), max(y_test))\n",
        "p2 = min(min(predictr), min(y_test))\n",
        "plt.plot([p1, p2], [p1, p2], 'b-')\n",
        "plt.xlabel('Observed', fontsize=12)\n",
        "plt.ylabel('Predicted', fontsize=12)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MUnhil7akWW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a plot of observed vs predicted values\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(y_test, predictr, c='crimson')\n",
        "\n",
        "p1 = max(max(predictr), max(y_test))\n",
        "p2 = min(min(predictr), min(y_test))\n",
        "plt.plot([p1, p2], [p1, p2], 'b-')\n",
        "plt.xlabel('Observed', fontsize=12)\n",
        "plt.ylabel('Predicted', fontsize=12)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LQYPuTCuTMJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **XGBoost**"
      ],
      "metadata": {
        "id": "OsaCbAOpUXbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set range for Number estimators\n",
        "n_estimators = [int(x) for x in np.linspace(start = 1000, stop = 10000, num = 10)]\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Learning rate investigated values\n",
        "learning_rate = [0.01,0.03,0.09,0.1,0.3]\n",
        "# colsample_by tree investigated values\n",
        "colsample_bytree = [0.3,0.7]\n",
        "\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_depth': max_depth,\n",
        "               'learning_rate': learning_rate,\n",
        "               'colsample_bytree': colsample_bytree}\n",
        "print(random_grid)"
      ],
      "metadata": {
        "id": "mnQjozJSlFQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "xgbr = xgb.XGBRegressor()\n",
        "# Random search of parameters, using 10 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "xgbr_random = RandomizedSearchCV(estimator = xgbr, param_distributions = random_grid, \n",
        "                               n_iter = 10, cv = 10, verbose=1, random_state=42, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "xgbr_random.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "j0qxnieSnThb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find best parameters from random search\n",
        "xgbr_random.best_params_"
      ],
      "metadata": {
        "id": "Z61e3wdJndKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "excNrGU_ZU4p"
      },
      "outputs": [],
      "source": [
        "#Run and fit the model\n",
        "xgbr = xgb.XGBRegressor(n_estimators=4000, max_depth=10,\n",
        "                       learning_rate=0.03,\n",
        "                       colsample_bytree = 0.3,\n",
        "                       random_state=42)\n",
        "xgbr.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Run and fit the model\n",
        "xgbr = xgb.XGBRegressor(n_estimators=10000, max_depth=9,\n",
        "                       learning_rate=0.03,\n",
        "                       colsample_bytree = 0.7,\n",
        "                       random_state=42)\n",
        "xgbr.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "Aikg0mlqeHBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqx0BaXxoY8k"
      },
      "outputs": [],
      "source": [
        "# generate Predictions\n",
        "predictx=xgbr.predict(X_test)\n",
        "predictx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVr1d73MUlBJ"
      },
      "outputs": [],
      "source": [
        "# Evaluating the Algorithm\n",
        "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictx))  \n",
        "print('Mean Squared Error:', metrics.mean_squared_error(y_test, predictx))  \n",
        "print('Root Mean Squared Error:', \n",
        "      np.sqrt(metrics.mean_squared_error(y_test, predictx)))\n",
        "print(\"R2: %.4f\" % r2_score(y_test, predictx))\n",
        "#print(xgbr.score(X_train, y_train))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xdf = pd.DataFrame(X)\n",
        "Xf = list(Xdf.columns)\n",
        "Xdf = pd.DataFrame(X)\n",
        "Xf = list(Xdf.columns)\n",
        "# Get numerical feature importances\n",
        "importances = list(xgbr.feature_importances_)\n",
        "# List of tuples with variable and importance\n",
        "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(Xf, importances)]\n",
        "# Sort the feature importances by most important first\n",
        "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
        "# Print out the feature and importances \n",
        "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
      ],
      "metadata": {
        "id": "rDoo0BKKkDgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CV score\n",
        "scores = cross_val_score(xgbr, X_train, y_train, \n",
        "                         scoring='r2', cv=10,n_jobs=-1)\n",
        "print(scores)\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "id": "4-0SiVCf5ph-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGB Plots"
      ],
      "metadata": {
        "id": "1ibp2lf3RhcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a plot of observed vs predicted values on a log scale\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(y_test, predictx, c='crimson')\n",
        "plt.yscale('log')\n",
        "plt.xscale('log')\n",
        "\n",
        "p1 = max(max(predictx), max(y_test))\n",
        "p2 = min(min(predictx), min(y_test))\n",
        "plt.plot([p1, p2], [p1, p2], 'b-')\n",
        "plt.xlabel('Observed', fontsize=12)\n",
        "plt.ylabel('Predicted', fontsize=12)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zUzJ5Z4lkYCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a plot of observed vs predicted values\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(y_test, predictx, c='crimson')\n",
        "\n",
        "p1 = max(max(predictx), max(y_test))\n",
        "p2 = min(min(predictx), min(y_test))\n",
        "plt.plot([p1, p2], [p1, p2], 'b-')\n",
        "plt.xlabel('Observed', fontsize=12)\n",
        "plt.ylabel('Predicted', fontsize=12)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kOg26HYvESsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sharston site**"
      ],
      "metadata": {
        "id": "H1xWLhBJbamu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering"
      ],
      "metadata": {
        "id": "lwnmxC9EaD4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#remove empty channel 1 data and change the date format to match that of the pollutants data\n",
        "traffic3 = traffic3[traffic3.LaneDescription != 'Channel 2']\n",
        "date_sr = pd.to_datetime(pd.Series(traffic3.date))\n",
        "traffic3.date = change_format = date_sr.dt.strftime('%d/%m/%Y %H:%M')\n",
        " \n",
        "# Print the formatted date\n",
        "print(change_format)"
      ],
      "metadata": {
        "id": "_ZsCm-ebliEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Megre the data on date\n",
        "df2 = pd.merge(traffic3,PollutantsShar[['date','NO2','wd','ws','temp','longitude', 'latitude']],\n",
        "               on='date', how='left')"
      ],
      "metadata": {
        "id": "rkxwRjh4MBcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whukBWf4r1Yv"
      },
      "outputs": [],
      "source": [
        "#Drop irrelevant null filled variables\n",
        "df2 = df2.drop(['AvgSpeed', 'PmlHGV', 'Flag Text'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check for missing values\n",
        "df2.isnull().mean()"
      ],
      "metadata": {
        "id": "6sIquy7yH1oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop missing values\n",
        "df2 = df2.dropna()"
      ],
      "metadata": {
        "id": "3apW2DIiQzgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Statistics summary of data\n",
        "df2[['NO2','Volume','ws','wd','temp']].describe()"
      ],
      "metadata": {
        "id": "R6BySzwGE7MD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Statistics summary of data\n",
        "PollutantsShar[['NO2','ws','wd','temp']].describe()"
      ],
      "metadata": {
        "id": "nuGDSCF7F0SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#New seasonality features extracted\n",
        "df2['date'] =  pd.to_datetime(df2['date'])\n",
        "df2['Year'] = df2['date'].dt.year\n",
        "df2['Month'] = df2['date'].dt.month\n",
        "df2['Hour'] = df2['date'].dt.hour\n",
        "df2['DayofWeek'] = df2['date'].dt.dayofweek\n",
        "df2['dayofyear'] = df2['date'].dt.dayofyear\n",
        "df2['weekofyear'] = df2['date'].dt.weekofyear\n",
        "df2['quarter'] = df2['date'].dt.quarter\n",
        "#rolling means features\n",
        "df2['NO2_moving_avg'] = df2['NO2'].rolling(window=3).mean()\n",
        "df2['NO2_moving_avg2'] = df2['NO2'].rolling(window=25).mean()\n",
        "df2['NO2_moving_avg3'] = df2['NO2'].rolling(window=50).mean()\n",
        "df2['NO2_moving_avg4'] = df2['NO2'].rolling(window=100).mean()"
      ],
      "metadata": {
        "id": "d2CE2ye6Mexc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Wind direction from degrees to textual\n",
        "df2['wd'] = df2['wd'].mask(df2['wd'] == 0.1, 0)\n",
        "df2['wd'] = df2['wd'].mask((df2['wd'] > 0.1) & (df2['wd'] < 90), 0.5)\n",
        "df2['wd'] = df2['wd'].mask(df2['wd'] == 90, 1)\n",
        "df2['wd'] = df2['wd'].mask((df2['wd'] > 90) & (df2['wd'] < 180), 1.5)\n",
        "df2['wd'] = df2['wd'].mask(df2['wd'] == 180, 2)\n",
        "df2['wd'] = df2['wd'].mask((df2['wd'] > 180) & (df2['wd'] < 270), 2.5)\n",
        "df2['wd'] = df2['wd'].mask(df2['wd'] == 270, 3)\n",
        "df2['wd'] = df2['wd'].mask((df2['wd'] > 270) & (df2['wd'] < 359.9), 3.5)\n",
        "df2['wd'] = df2['wd'].mask(df2['wd'] == 359.9, 'North')\n",
        "\n",
        "df2['wd'] = df2['wd'].mask(df2['wd'] == 0, 'North')\n",
        "df2['wd'] = df2['wd'].mask(df2['wd'] == 0.5, 'North East')\n",
        "df2['wd'] = df2['wd'].mask(df2['wd'] == 1, 'East')\n",
        "df2['wd'] = df2['wd'].mask(df2['wd'] == 1.5, 'South East')\n",
        "df2['wd'] = df2['wd'].mask(df2['wd'] == 2, 'South')\n",
        "df2['wd'] = df2['wd'].mask(df2['wd'] == 2.5, 'South West')\n",
        "df2['wd'] = df2['wd'].mask(df2['wd'] == 3, 'West')\n",
        "df2['wd'] = df2['wd'].mask(df2['wd'] == 3.5, 'North West')"
      ],
      "metadata": {
        "id": "6gaD_0xiG406"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating instance of one-hot-encoder\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "#perform one-hot encoding on 'wd' column \n",
        "encoder_df = pd.DataFrame(encoder.fit_transform(df2[['wd']]).toarray())\n",
        "\n",
        "#merge one-hot encoded columns back with original DataFrame\n",
        "df2 = df2.join(encoder_df)"
      ],
      "metadata": {
        "id": "kM29K6U8G7aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating instance of one-hot-encoder\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "#perform one-hot encoding on 'dayofweek' column and add them to new columns\n",
        "df2[['ohe1','ohe2','ohe3','oh4','o5','o6','o7']] = encoder.fit_transform(df2[['DayofWeek']]).toarray()"
      ],
      "metadata": {
        "id": "8GZl1OTRG-ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extract new seasonality features\n",
        "PollutantsShar['date'] =  pd.to_datetime(PollutantsShar['date'])\n",
        "PollutantsShar['Year'] = PollutantsShar['date'].dt.year\n",
        "PollutantsShar['Month'] = PollutantsShar['date'].dt.month\n",
        "PollutantsShar['Hour'] = PollutantsShar['date'].dt.hour\n",
        "PollutantsShar['DayofWeek'] = PollutantsShar['date'].dt.dayofweek\n",
        "PollutantsShar['dayofyear'] = PollutantsShar['date'].dt.dayofyear\n",
        "PollutantsShar['weekofyear'] = PollutantsShar['date'].dt.weekofyear\n",
        "PollutantsShar['quarter'] = PollutantsShar['date'].dt.quarter\n",
        "PollutantsShar['Day'] = PollutantsShar['date'].dt.day\n",
        "#new rollings means variables\n",
        "PollutantsShar['NO2_moving_avg'] = PollutantsShar['NO2'].rolling(window=3).mean()\n",
        "PollutantsShar['NO2_moving_avg2'] = PollutantsShar['NO2'].rolling(window=25).mean()\n",
        "PollutantsShar['NO2_moving_avg3'] = PollutantsShar['NO2'].rolling(window=50).mean()\n",
        "PollutantsShar['NO2_moving_avg4'] = PollutantsShar['NO2'].rolling(window=100).mean()"
      ],
      "metadata": {
        "id": "y7H_kUpQJ5Ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check for missing values\n",
        "PollutantsShar.isnull().mean()"
      ],
      "metadata": {
        "id": "vktazqJiIEM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop missing values\n",
        "PollutantsShar = PollutantsShar.dropna()"
      ],
      "metadata": {
        "id": "FXFdq7oRKjX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wind direction from degrees to textual\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask(PollutantsShar['wd'] == 0, 0)\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask((PollutantsShar['wd'] > 0) & (PollutantsShar['wd'] < 90), 0.5)\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask(PollutantsShar['wd'] == 90, 1)\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask((PollutantsShar['wd'] > 90) & (PollutantsShar['wd'] < 180), 1.5)\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask(PollutantsShar['wd'] == 180, 2)\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask((PollutantsShar['wd'] > 180) & (PollutantsShar['wd'] < 270), 2.5)\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask(PollutantsShar['wd'] == 270, 3)\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask((PollutantsShar['wd'] > 270) & (PollutantsShar['wd'] < 360), 3.5)\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask(PollutantsShar['wd'] == 360, 'North')\n",
        "\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask(PollutantsShar['wd'] == 0, 'North')\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask(PollutantsShar['wd'] == 0.5, 'North East')\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask(PollutantsShar['wd'] == 1, 'East')\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask(PollutantsShar['wd'] == 1.5, 'South East')\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask(PollutantsShar['wd'] == 2, 'South')\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask(PollutantsShar['wd'] == 2.5, 'South West')\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask(PollutantsShar['wd'] == 3, 'West')\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask(PollutantsShar['wd'] == 3.5, 'North West')"
      ],
      "metadata": {
        "id": "m8JwOASv4qrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating instance of one-hot-encoder\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "#perform one-hot encoding on 'wd' column \n",
        "encoder_df = pd.DataFrame(encoder.fit_transform(PollutantsShar[['wd']]).toarray())\n",
        "\n",
        "#merge one-hot encoded columns back with original DataFrame\n",
        "PollutantsShar = PollutantsShar.join(encoder_df)"
      ],
      "metadata": {
        "id": "BERM3ufkGgtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating instance of one-hot-encoder\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "#perform one-hot encoding on 'dayofweek' column \n",
        "PollutantsShar[['ohe1','ohe2','ohe3','oh4','o5','o6','o7']] = encoder.fit_transform(PollutantsShar[['DayofWeek']]).toarray()"
      ],
      "metadata": {
        "id": "BlJrixOZGhXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hG_4q8nnrH7"
      },
      "source": [
        "## **Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Data Where X contains the predictor variables and y the target"
      ],
      "metadata": {
        "id": "kZ7ZZzlakDZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb1"
      ],
      "metadata": {
        "id": "f-jB73Y4xB1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = PollutantsShar.iloc[:, [5,6,7]].values\n",
        "y = PollutantsShar.iloc[:,3].values"
      ],
      "metadata": {
        "id": "xveglSlFKyJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OHE\n",
        "X = PollutantsShar.iloc[:, [6,7,26,27,28,29,30,31,32]].values\n",
        "y = PollutantsShar.iloc[:,3].values"
      ],
      "metadata": {
        "id": "OWx4A8WkxK77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb2"
      ],
      "metadata": {
        "id": "-YBIWgtWxCWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = PollutantsShar.iloc[:, [5,6,7,14,15,16,17,18,19,20]].values\n",
        "y = PollutantsShar.iloc[:,3].values"
      ],
      "metadata": {
        "id": "mWZHtjjBPFtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OHE\n",
        "X = PollutantsShar.iloc[:, [6,7,14,15,16,17,18,19,20,26,27,28,29,30,31,32,33,34,35,36,37,38,39]].values\n",
        "y = PollutantsShar.iloc[:,3].values"
      ],
      "metadata": {
        "id": "avPhDu0EyWYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb3"
      ],
      "metadata": {
        "id": "Y935VfW_xDEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = df2.iloc[:, [6,9,10,11,14,15,16,17,18,19,20]].values\n",
        "y = df2.iloc[:,8].values"
      ],
      "metadata": {
        "id": "t1QyGvWZpukF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OHE\n",
        "X = df2.iloc[:, [6,10,11,14,15,16,17,18,19,20,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40]].values\n",
        "y = df2.iloc[:,8].values"
      ],
      "metadata": {
        "id": "1oNlPp8H0sPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb4"
      ],
      "metadata": {
        "id": "qGRXrGy3xEAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = PollutantsShar.iloc[:, [5,6,7,14,15,16,17,18,19,20,21]].values\n",
        "y = PollutantsShar.iloc[:,22].values"
      ],
      "metadata": {
        "id": "8ecVvgYqa57j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OHE\n",
        "X = PollutantsShar.iloc[:, [6,7,14,15,16,17,18,19,20,26,27,28,29,30,31,32,33,34,35,36,37,38,39]].values\n",
        "y = PollutantsShar.iloc[:,22].values"
      ],
      "metadata": {
        "id": "soW4DS541I3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb5"
      ],
      "metadata": {
        "id": "b0R5cjjIxE1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = PollutantsShar.iloc[:, [5,6,7,14,15,16,17,18,19,20]].values\n",
        "y = PollutantsShar.iloc[:,23].values\n",
        "#dependant variable is now the rolling mean(25)"
      ],
      "metadata": {
        "id": "hNV-le1ia3rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OHE\n",
        "X = PollutantsShar.iloc[:, [6,7,14,15,16,17,18,19,20,26,27,28,29,30,31,32,33,34,35,36,37,38,39]].values\n",
        "y = PollutantsShar.iloc[:,23].values"
      ],
      "metadata": {
        "id": "TyuD-hO31fwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb6"
      ],
      "metadata": {
        "id": "NeOz158ExFpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = PollutantsShar.iloc[:, [5,6,7,14,15,16,17,18,19,20]].values\n",
        "y = PollutantsShar.iloc[:,24].values"
      ],
      "metadata": {
        "id": "G4cxQnXOPVS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OHE\n",
        "X = PollutantsShar.iloc[:, [6,7,14,15,16,17,18,19,20,26,27,28,29,30,31,32,33,34,35,36,37,38,39]].values\n",
        "y = PollutantsShar.iloc[:,24].values"
      ],
      "metadata": {
        "id": "xjH-yDlr10cA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb7"
      ],
      "metadata": {
        "id": "6F84nfMZxGYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = df2.iloc[:, [6,9,10,11,14,15,16,17,18,19,20]].values\n",
        "y = df2.iloc[:,23].values"
      ],
      "metadata": {
        "id": "R8mf9mAIO3Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OHE\n",
        "X = PollutantsShar.iloc[:, [6,10,11,14,15,16,17,18,19,20,26,27,28,29,30,31,32,33,34,35,36,37,38,39]].values\n",
        "y = PollutantsShar.iloc[:,23].values"
      ],
      "metadata": {
        "id": "srOACLLZtXW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perform Train test split"
      ],
      "metadata": {
        "id": "v6trKZUfI9pD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4yjPwsA3uxN"
      },
      "outputs": [],
      "source": [
        "\n",
        "scaler = StandardScaler()\n",
        "print(scaler.fit(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdxiZ4NaoGBe"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1,random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "AWsKhxP0NE6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "print(random_grid)"
      ],
      "metadata": {
        "id": "9ayhUZmkkhyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "rf = RandomForestRegressor()\n",
        "# Random search of parameters, using 10 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n",
        "                               n_iter = 10, cv = 10, verbose=1, random_state=42, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "rf_random.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "6D-yfyzykiXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find best parameters\n",
        "rf_random.best_params_"
      ],
      "metadata": {
        "id": "f-ofxhiJklMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run and fit the model fro Varcomb1\n",
        "rfregressor = RandomForestRegressor(n_estimators = 200,max_depth = 10, \n",
        "                                  max_features= 'sqrt', min_samples_split = 5, min_samples_leaf= 2,\n",
        "                                  bootstrap = True, random_state = 42)\n",
        "rfregressor.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "aigSSVK6ZpIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdKsKsr03_jR"
      },
      "outputs": [],
      "source": [
        "#Run and fit the model for Varcomb2\n",
        "rfregressor = RandomForestRegressor(n_estimators = 400,max_depth = 60, \n",
        "                                  max_features= 'sqrt', min_samples_split = 10, min_samples_leaf= 1,\n",
        "                                  bootstrap = False, random_state = 42)\n",
        "rfregressor.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VV1_OXqEoS1u"
      },
      "outputs": [],
      "source": [
        "# Prediction\n",
        "predictr=rfregressor.predict(X_test)\n",
        "predictr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmK7HTEwoqa7"
      },
      "outputs": [],
      "source": [
        "#Evaluation\n",
        "print(\"MSE: %.4f\" % mean_squared_error(y_test, predictr))\n",
        "print(\"RMSE: %.4f\" % math.sqrt(mean_squared_error(y_test, predictr)))\n",
        "print(\"MAE: %.4f\" % mean_absolute_error(y_test, predictr))\n",
        "print(\"R2: %.4f\" % r2_score(y_test, predictr))\n",
        "print(rfregressor.score(X_train, y_train))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xdf = pd.DataFrame(X)\n",
        "Xf = list(Xdf.columns)\n",
        "Xdf = pd.DataFrame(X)\n",
        "Xf = list(Xdf.columns)\n",
        "# Get numerical feature importances\n",
        "importances = list(xgbr.feature_importances_)\n",
        "# List of tuples with variable and importance\n",
        "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(Xf, importances)]\n",
        "# Sort the feature importances by most important first\n",
        "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
        "# Print out the feature and importances \n",
        "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
      ],
      "metadata": {
        "id": "oVokztqhk4C5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CV score\n",
        "scores = cross_val_score(rfregressor, X_train, y_train, \n",
        "                         scoring='r2', cv=10,n_jobs=-1)\n",
        "print(scores)\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "id": "FVvm32sbeOM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RF plots"
      ],
      "metadata": {
        "id": "sK3-zAWFF7gq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#generate a plot of observed vs predicted values on a log scale\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(y_test, predictr, c='crimson')\n",
        "plt.yscale('log')\n",
        "plt.xscale('log')\n",
        "\n",
        "p1 = max(max(predictr), max(y_test))\n",
        "p2 = min(min(predictr), min(y_test))\n",
        "plt.plot([p1, p2], [p1, p2], 'b-')\n",
        "plt.xlabel('Observed', fontsize=12)\n",
        "plt.ylabel('Predicted', fontsize=12)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y1nBWGx9kq3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate a plot of observed vs predicted values\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(y_test, predictr, c='crimson')\n",
        "\n",
        "\n",
        "p1 = max(max(predictr), max(y_test))\n",
        "p2 = min(min(predictr), min(y_test))\n",
        "plt.plot([p1, p2], [p1, p2], 'b-')\n",
        "plt.xlabel('Observed', fontsize=12)\n",
        "plt.ylabel('Predicted', fontsize=12)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-eaaCs6KxUdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost"
      ],
      "metadata": {
        "id": "l5UrwIbFNPky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of iterations in XGBoost\n",
        "n_estimators = [int(x) for x in np.linspace(start = 2000, stop = 20000, num = 10)]\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Learning rate\n",
        "learning_rate = [0.01,0.03,0.09,0.1,0.3]\n",
        "# colsample_bytree\n",
        "colsample_bytree = [0.3,0.7]\n",
        "\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_depth': max_depth,\n",
        "               'learning_rate': learning_rate,\n",
        "               'colsample_bytree': colsample_bytree}\n",
        "print(random_grid)"
      ],
      "metadata": {
        "id": "OhO5eYginyJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "xgb = xgb.XGBRegressor()\n",
        "# Random search of parameters, using 10 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "xgb_random = RandomizedSearchCV(estimator = xgb, param_distributions = random_grid, \n",
        "                               n_iter = 10, cv = 10, verbose=1, random_state=42, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "xgb_random.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "5GVeysstnymm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find best parameters\n",
        "xgb_random.best_params_"
      ],
      "metadata": {
        "id": "Khca8n_1n76b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run fit the model \n",
        "xgbr = xgb.XGBRegressor(n_estimators=4000, max_depth=10,\n",
        "                       learning_rate=0.03,\n",
        "                       colsample_bytree = 0.3,\n",
        "                       random_state=42)\n",
        "xgbr.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "9V9evlIa4LV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run and fit the model\n",
        "xgbr = xgb.XGBRegressor(n_estimators=10000, max_depth=9,\n",
        "                       learning_rate=0.03,\n",
        "                       colsample_bytree = 0.7,\n",
        "                       random_state=42)\n",
        "xgbr.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "DN_yfjywNT6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuPbwpQgp4k-"
      },
      "outputs": [],
      "source": [
        "# Prediction\n",
        "predictx=xgbr.predict(X_test)\n",
        "predictx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZCb6HCap4mB"
      },
      "outputs": [],
      "source": [
        "print(\"MSE: %.4f\" % mean_squared_error(y_test, predictx))\n",
        "print(\"RMSE: %.4f\" % math.sqrt(mean_squared_error(y_test, predictx)))\n",
        "print(\"MAE: %.4f\" % mean_absolute_error(y_test, predictx))\n",
        "print(\"R2: %.4f\" % r2_score(y_test, predictx))\n",
        "#print(xgbr.score(X_train, y_train))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xdf = pd.DataFrame(X)\n",
        "Xf = list(Xdf.columns)\n",
        "Xdf = pd.DataFrame(X)\n",
        "Xf = list(Xdf.columns)\n",
        "# Get numerical feature importances\n",
        "importances = list(xgbr.feature_importances_)\n",
        "# List of tuples with variable and importance\n",
        "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(Xf, importances)]\n",
        "# Sort the feature importances by most important first\n",
        "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
        "# Print out the feature and importances \n",
        "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
      ],
      "metadata": {
        "id": "eMXpWdmMk5hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CV score\n",
        "scores = cross_val_score(xgbr, X_train, y_train, \n",
        "                         scoring='r2', cv=10,n_jobs=-1)\n",
        "print(scores)\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "id": "r3kSC2Vb4R6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### xgb plots"
      ],
      "metadata": {
        "id": "MivsA36-F_P8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#generate a plot of observed vs predicted values on a log scale\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(y_test, predictx, c='crimson')\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "\n",
        "p1 = max(max(predictx), max(y_test))\n",
        "p2 = min(min(predictx), min(y_test))\n",
        "plt.plot([p1, p2], [p1, p2], 'b-')\n",
        "plt.xlabel('Observed', fontsize=12)\n",
        "plt.ylabel('Predicted', fontsize=12)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-UT00r4FkusQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate a plot of observed vs predicted values\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(y_test, predictx, c='crimson')\n",
        "\n",
        "\n",
        "p1 = max(max(predictx), max(y_test))\n",
        "p2 = min(min(predictx), min(y_test))\n",
        "plt.plot([p1, p2], [p1, p2], 'b-')\n",
        "plt.xlabel('Observed', fontsize=12)\n",
        "plt.ylabel('Predicted', fontsize=12)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uVhrWkYvxv2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Picc&Shar **EDA**"
      ],
      "metadata": {
        "id": "xlhjA2YwuBKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#figure of rolling means NO2 at 200 windows where the xaxis represent the number of observations\n",
        "fig= plt.figure(figsize=(10,5))\n",
        "PollutantsPicc['NO2'].plot(figsize=(10,6))\n",
        "PollutantsPicc['NO2'].rolling(window=200).mean().plot()\n",
        "plt.title('Piccadilly')\n",
        "plt.ylabel('NO2')"
      ],
      "metadata": {
        "id": "3Xk8II1bTiLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#figure of rolling means NO2 at 200 windows where the xaxis represent the number of observations\n",
        "fig= plt.figure(figsize=(10,5))\n",
        "PollutantsShar['NO2'].plot(figsize=(10,6))\n",
        "PollutantsShar['NO2'].rolling(window=200).mean().plot()\n",
        "plt.ylabel('NO2')\n",
        "plt.title('Sharston')"
      ],
      "metadata": {
        "id": "1azQcyJ_RXhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Wind rose distribution\n",
        "ax = WindroseAxes.from_ax()\n",
        "ax.bar(PollutantsShar.wd, PollutantsShar.ws, normed=True, opening=0.8, edgecolor='white')\n",
        "ax.set_legend()\n",
        "ax.legend(loc='upper left')\n",
        "ax.set_title('Sharston', loc='right')\n",
        "ax = WindroseAxes.from_ax()\n",
        "ax.bar(PollutantsPicc.wd, PollutantsPicc.ws, normed=True, opening=0.8, edgecolor='white')\n",
        "ax.set_legend()\n",
        "ax.legend(loc='upper left')\n",
        "ax.set_title('Piccadilly', loc='right')"
      ],
      "metadata": {
        "id": "UAYrLtYhiOmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#figure of density distribution of NO2 and traffic volume\n",
        "fig, axes = plt.subplots(2, 1, figsize=(12,6))\n",
        "sb.kdeplot(PollutantsPicc.NO2 , bw = 0.5, ax=axes[0], label='Piccadilly')\n",
        "sb.kdeplot(df.Volume , bw = 0.5, ax=axes[1], label='Piccadilly')\n",
        "sb.kdeplot(PollutantsShar.NO2 , bw = 0.5, ax=axes[0], label='Sharston')\n",
        "sb.kdeplot(df2.Volume , bw = 0.5, ax=axes[1], label='Sharston')\n",
        "# labels\n",
        "axes[0].set_xlabel('NO2')\n",
        "axes[0].legend(loc='upper right')\n",
        "axes[1].legend(loc='upper right')\n",
        "axes[1].set_xlabel('Traffic Volume')\n",
        "axes[0].axvline(x=PollutantsPicc.NO2.median(),\n",
        "            color='blue',\n",
        "            ls='--', \n",
        "            lw=2.5)\n",
        "axes[0].axvline(x=PollutantsShar.NO2.median(),\n",
        "            color='orange',\n",
        "            ls='--', \n",
        "            lw=2.5)\n",
        "\n",
        "axes[0].axvline(x=PollutantsPicc.NO2.mean(),\n",
        "            color='violet',\n",
        "            ls='--', \n",
        "            lw=2.5)\n",
        "axes[0].axvline(x=PollutantsShar.NO2.mean(),\n",
        "            color='red',\n",
        "            ls='--', \n",
        "            lw=2.5)\n",
        "\n",
        "\n",
        "axes[1].axvline(x=df.Volume.median(),\n",
        "            color='blue',\n",
        "            ls='--', \n",
        "            lw=2.5)\n",
        "axes[1].axvline(x=df2.Volume.median(),\n",
        "            color='orange',\n",
        "            ls='--', \n",
        "            lw=2.5)\n",
        "\n",
        "axes[1].axvline(x=df.Volume.mean(),\n",
        "            color='violet',\n",
        "            ls='--', \n",
        "            lw=2.5)\n",
        "axes[1].axvline(x=df2.Volume.mean(),\n",
        "            color='red',\n",
        "            ls='--', \n",
        "            lw=2.5)\n",
        "plt.subplots_adjust(hspace=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9_GscvjfIANk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(PollutantsPicc.NO2.median())\n",
        "print(PollutantsPicc.NO2.mean())\n",
        "print(df.Volume.mean())\n",
        "print(df.Volume.median())"
      ],
      "metadata": {
        "id": "EXPBEjwdeM_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(PollutantsShar.NO2.median())\n",
        "print(PollutantsShar.NO2.mean())\n",
        "print(df2.Volume.mean())\n",
        "print(df2.Volume.median())"
      ],
      "metadata": {
        "id": "rrj7NnGNekv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PollutantsShar.NO2.describe()"
      ],
      "metadata": {
        "id": "aNqkQe9EH2Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(PollutantsPicc.NO2.mode())\n",
        "print(df.Volume.mode())"
      ],
      "metadata": {
        "id": "YCp0jxo5fbBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(PollutantsShar.NO2.mode())\n",
        "print(df2.Volume.mode())"
      ],
      "metadata": {
        "id": "AuA8gCsae5lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#figure of temperature distribution\n",
        "fig, axes = plt.subplots(2, 1, figsize=(10,4), sharex=True)\n",
        "sns.boxplot(y=PollutantsPicc.site, x=PollutantsPicc.temp, ax=axes[0],palette=\"Set2\")\n",
        "sns.boxplot(y=PollutantsShar.site, x=PollutantsShar.temp, ax=axes[1])\n",
        "axes[0].set_xlabel('')\n",
        "axes[0].set_ylabel('Piccadilly')\n",
        "axes[1].set_ylabel('Sharston')\n",
        "# Hide X and Y axes label marks\n",
        "axes[0].yaxis.set_tick_params(labelleft=False)\n",
        "axes[1].yaxis.set_tick_params(labelleft=False)\n",
        "\n",
        "# Hide X and Y axes tick marks\n",
        "axes[0].set_yticks([])\n",
        "axes[1].set_yticks([])\n",
        "axes[1].set_xlabel('Temperature (Celcius)')\n",
        "axes[0].label_outer()\n",
        "axes[1].label_outer()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUO1bfMTmE__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#figure of wind speed distribution\n",
        "fig, axes = plt.subplots(2, 1, figsize=(10,4), sharex=True)\n",
        "sns.boxplot(y=PollutantsPicc.site, x=PollutantsPicc.ws, ax=axes[0],palette=\"Set2\")\n",
        "sns.boxplot(y=PollutantsShar.site, x=PollutantsShar.ws, ax=axes[1])\n",
        "axes[0].set_xlabel('')\n",
        "axes[1].set_xlabel('wind speed (m/s)')\n",
        "axes[0].set_ylabel('Piccadilly')\n",
        "axes[1].set_ylabel('Sharston')\n",
        "# Hide X and Y axes label marks\n",
        "axes[0].yaxis.set_tick_params(labelleft=False)\n",
        "axes[1].yaxis.set_tick_params(labelleft=False)\n",
        "\n",
        "# Hide X and Y axes tick marks\n",
        "axes[0].set_yticks([])\n",
        "axes[1].set_yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JbIZK6w0nxTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(4, 2, figsize=(15,21))\n",
        "\n",
        "sns.lineplot(x='Hour', y='NO2', data=PollutantsPicc, ax=axes[0,0], label='Piccadilly')\n",
        "sns.lineplot(x='Hour', y='NO2', data=PollutantsShar, ax=axes[0,0], label='Sharston')\n",
        "\n",
        "sns.lineplot(x='Year', y='NO2', data=PollutantsPicc, ax=axes[1,0], label='Piccadilly')\n",
        "sns.lineplot(x='Year', y='NO2', data=PollutantsShar, ax=axes[1,0], label='Sharston')\n",
        "\n",
        "\n",
        "sns.lineplot(x='DayofWeek', y='NO2', data=PollutantsPicc, ax=axes[2,0], label='Piccadilly')\n",
        "sns.lineplot(x='DayofWeek', y='NO2', data=PollutantsShar, ax=axes[2,0], label='Sharston')\n",
        "\n",
        "sns.lineplot(x='Month', y='NO2', data=PollutantsPicc, ax=axes[3,0], label='Piccadilly')\n",
        "sns.lineplot(x='Month', y='NO2', data=PollutantsShar, ax=axes[3,0], label='Sharston')\n",
        "\n",
        "\n",
        "\n",
        "sns.lineplot(x='Hour', y='Volume', data=df, ax=axes[0,1], label='Piccadilly')\n",
        "sns.lineplot(x='Hour', y='Volume', data=df2, ax=axes[0,1], label='Sharston')\n",
        "\n",
        "sns.lineplot(x='Year', y='Volume', data=df, ax=axes[1,1], label='Piccadilly')\n",
        "sns.lineplot(x='Year', y='Volume', data=df2, ax=axes[1,1], label='Sharston')\n",
        "\n",
        "sns.lineplot(x='DayofWeek', y='Volume', data=df, ax=axes[2,1], label='Piccadilly')\n",
        "sns.lineplot(x='DayofWeek', y='Volume', data=df2, ax=axes[2,1], label='Sharston')\n",
        "\n",
        "sns.lineplot(x='Month', y='Volume', data=df, ax=axes[3,1], label='Piccadilly')\n",
        "sns.lineplot(x='Month', y='Volume', data=df2, ax=axes[3,1], label='Sharston')\n",
        "\n",
        "#Axes configuration ----\n",
        "\n",
        "\n",
        "axes[0,0].legend(loc='upper right')\n",
        "axes[1,0].legend(loc='upper right')\n",
        "axes[2,0].legend(loc='upper right')\n",
        "axes[3,0].legend(loc='upper right')\n",
        "\n",
        "\n",
        "\n",
        "axes[0,1].legend(loc='upper right')\n",
        "axes[1,1].legend(loc='upper right')\n",
        "axes[2,1].legend(loc='upper right')\n",
        "axes[3,1].legend(loc='upper right')\n",
        "\n",
        "\n",
        "axes[0,0].set_xlabel('Hour')\n",
        "axes[0,0].set_title('Hourly NO2 concentrations',loc = 'center')\n",
        "axes[0,1].set_xlabel('Hour')\n",
        "axes[0,1].set_title('Hourly Traffic Volume',loc = 'center')\n",
        "axes[0,1].set_ylabel('Traffic Volume')\n",
        "\n",
        "axes[1,0].set_title('Yearly NO2 concentrations',loc = 'center')\n",
        "axes[1,1].set_title('Yearly Tarffic Volume',loc = 'center')\n",
        "axes[1,1].set_ylabel('Traffic Volume')\n",
        "axes[1,1].set_xticks([2016,2017,2018,2019,2020,2021])\n",
        "\n",
        "axes[2,0].set_xlabel('Day of Week')\n",
        "axes[2,0].set_title('NO2 concentrations Daily throughout a week',loc = 'center')\n",
        "axes[2,0].set_xticks([0,1,2,3,4,5,6])\n",
        "axes[2,0].set_xticklabels(['Mon','Tue','Wed','Thu','Fri','Sat','Sun'])\n",
        "axes[2,1].set_xlabel('Day of Week')\n",
        "axes[2,1].set_title('Traffic Volume Daily throughout a week',loc = 'center')\n",
        "axes[2,1].set_ylabel('Traffic Volume')\n",
        "axes[2,1].set_xticks([0,1,2,3,4,5,6])\n",
        "axes[2,1].set_xticklabels(['Mon','Tue','Wed','Thu','Fri','Sat','Sun'])\n",
        "\n",
        "axes[3,0].set_xlabel('Month')\n",
        "axes[3,0].set_title('Monthly NO2 concentrations',loc = 'center')\n",
        "axes[3,0].set_xticks([1,2,3,4,5,6,7,8,9,10,11,12])\n",
        "axes[3,0].set_xticklabels(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'])\n",
        "axes[3,1].set_xlabel('Month')\n",
        "axes[3,1].set_title('Monthly Traffic Volume',loc = 'center')\n",
        "axes[3,1].set_ylabel('Traffic Volume')\n",
        "axes[3,1].set_xticks([1,2,3,4,5,6,7,8,9,10,11,12])\n",
        "axes[3,1].set_xticklabels(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'])\n",
        "\n",
        "\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wpP-WEIgy11v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot autocorrelation function\n",
        "fig= plt.figure(figsize=(10,5))\n",
        "tsaplots.plot_acf(PollutantsPicc.NO2, lags=60, color='g',  title='Autocorrelation function (Piccadilly)')\n",
        "plt.xlabel('lags')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w9IpyErTmSeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot autocorrelation function\n",
        "fig= plt.figure(figsize=(10,5))\n",
        "tsaplots.plot_acf(PollutantsPicc.NO2, lags=4000, color='g',  title='Autocorrelation function (Piccadilly)')\n",
        "plt.xlabel('lags')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "75XcM-XUNTIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot autocorrelation function\n",
        "fig= plt.figure(figsize=(10,5))\n",
        "tsaplots.plot_acf(PollutantsPicc.NO2, lags=40000, color='g', title='Autocorrelation function (Piccadilly)')\n",
        "plt.xlabel('lags')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BhIVNO4woMgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot autocorrelation function\n",
        "fig= plt.figure(figsize=(10,5))\n",
        "tsaplots.plot_acf(PollutantsShar.NO2, lags=60, color='g', title='Autocorrelation function (Sharston)')\n",
        "plt.xlabel('lags')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qSGFjOGImavP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot autocorrelation function\n",
        "fig= plt.figure(figsize=(10,5))\n",
        "tsaplots.plot_acf(PollutantsShar.NO2, lags=4000, color='g', title='Autocorrelation function (Sharston)')\n",
        "plt.xlabel('lags')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H54I2cOdNI3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot autocorrelation function\n",
        "fig= plt.figure(figsize=(10,5))\n",
        "tsaplots.plot_acf(PollutantsShar.NO2, lags=40000, color='g', title='Autocorrelation function (Sharston)')\n",
        "plt.xlabel('lags')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xN04IspMoTmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "fig= plt.figure(figsize=(12,8))\n",
        "sns.boxplot(PollutantsPicc.wd, PollutantsPicc.NO2)\n",
        "plt.xlabel('Wind Direction')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D096pZjN33YW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "fig= plt.figure(figsize=(12,8))\n",
        "sns.boxplot(PollutantsShar.wd, PollutantsShar.NO2)\n",
        "plt.xlabel('Wind Direction')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tDovT1-H6eNN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "FwN5oQ8Valzz",
        "3BWdbgujbiku",
        "sPNNqTk57k4S",
        "mm8fTYm_p9PF",
        "cy2KqVvbTXzk",
        "6Pm-RE4EThm0",
        "q7Rvxc0OXt_R",
        "ikbjCk0ZAv6c",
        "1ibp2lf3RhcD",
        "H1xWLhBJbamu",
        "kZ7ZZzlakDZu",
        "f-jB73Y4xB1d",
        "-YBIWgtWxCWG",
        "Y935VfW_xDEO",
        "qGRXrGy3xEAa",
        "b0R5cjjIxE1U",
        "NeOz158ExFpp",
        "6F84nfMZxGYG",
        "mfbB3xZTxHA8",
        "v6trKZUfI9pD",
        "AWsKhxP0NE6_",
        "l5UrwIbFNPky",
        "MivsA36-F_P8",
        "xlhjA2YwuBKU"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}